{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Methods\n",
    "### Activity 2: General Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load the data into a pandas dataframe and display it. Our data is from a scientific paper from 1995 comparing levels of various chemicals to the age of carpets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import pandas as pd\n",
    "df = pd.read_csv('carpet_age.csv')\n",
    "display(df)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll extract the data into numpy arrays for regression. We'll also do some light data cleaning to remove data points with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# We have to extract the data from the columns we're interested in.\n",
    "# X will collect the independent variables, and y the dependent (response) variable.\n",
    "\n",
    "X_data = np.array(df[['cys_acid', 'cys', 'met', 'tyr']].values)\n",
    "y_data = np.array(df[['age']].values.flatten())\n",
    "\n",
    "# Notice that the bottom two entries don't have ages. To keep things simple we'll remove them.\n",
    "# Numpy's delete operation doesn't actually modify the arrays it operates it, it just returns a new array.\n",
    "# So we create two new arrays to do our regression with.\n",
    "y = np.delete(y_data,[len(y_data)-1, len(y_data)-2],0)\n",
    "X = np.delete(X_data,[len(X_data)-1, len(X_data)-2],0)\n",
    "\n",
    "print(X)\n",
    "print(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the parameters of the regression hyperplane $a_0 + a_1x_1 + a_2x_2 + a_3x_3 + a_4x_4$. This is done using scikit-learn's LinearRegression class, just like for simple linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "# We run the 'fit' function on X and y to get our regression model.\n",
    "model.fit(X,y)\n",
    "\n",
    "# intercept_ returns a_0, and coef_ returns [a_1,a_2,a_3,a_4], so we get out array of parameters by combining these\n",
    "A = np.append(model.intercept_, model.coef_) \n",
    "\n",
    "print('parameters:', A)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again just like with simple linear regression, we can easily find the coefficient of determination $r^2$ here too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = round(model.score(X, y),3)\n",
    "print('coefficient of determination:', r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that takes as input the array of data for the independent variables ($X$), and the array of data for the \n",
    "dependent vaiable ($y$), and also the array of parameters calculated for the multiple regression model (for us this is\n",
    "$A = [a_0,a_1,a_2,a_3,a_4]$), and outputs the correct value for the standard error of the estimate $s_{yx}$ (I think it should \n",
    "be 46.379)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def findSE(x_values, y_values, A):\n",
    "    #TODO\n",
    "    return \n",
    "\n",
    "s_yx = findSE(X,y,A)\n",
    "print('standard error of estimate:', s_yx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression we've just done involves 4 independent variables, so the regression hypersurface is a 4 dimensional shape in 5 dimensional space. This is hard to draw. To get a nice picture we'll restrict to just the first two independent vatriiables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2 = np.array(df[['cys_acid', 'cys']].values)[:-2]\n",
    "\n",
    "print(X_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can proceed like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_2,y)\n",
    "\n",
    "# Here A_2 will be [a_0,a_1,a_2]\n",
    "A_2 = np.append(model.intercept_, model.coef_) \n",
    "\n",
    "print('parameters: {}'.format(A_2))\n",
    "\n",
    "r2_new = round(model.score(X_2, y),3)\n",
    "print('coefficient of determination: {}'.format(r2_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also draw a picture. You should be able to drag the image to change the view. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import LinearLocator\n",
    "\n",
    "x1_values = np.delete(X_2,1,1)\n",
    "x2_values = np.delete(X_2,0,1)\n",
    "z_values = y\n",
    "x_line = np.linspace(min(x1_values), max(x1_values) + 1, 100)\n",
    "y_line = np.linspace(min(x2_values), max(x2_values) + 1, 100)\n",
    "X, Y = np.meshgrid(x_line, y_line)\n",
    "Z = A_2[0] + X*A_2[1] + Y*A_2[2]\n",
    "colours =[]\n",
    "for i in range(len(x1_values)):\n",
    "    if z_values[i] > A_2[0] + x1_values[i]*A_2[1] + x2_values[i]*A_2[2]:\n",
    "        colours.append('red')\n",
    "    else:\n",
    "        colours.append('blue')\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.w_zaxis.set_major_locator(LinearLocator(6))\n",
    "ax.scatter(x1_values, x2_values, z_values, marker = 'o', c = colours)\n",
    "ax.plot_surface(X, Y, Z, cmap=cm.viridis, linewidth=0, alpha = 0.5)\n",
    "ax.view_init(elev=10., azim=25)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write our own function for performing multiple regression with 2 independent $x$ variables. This function will take an 'array of arrays' representing the values of 2 independent $x$ variables, and an array of values of $y$, and it will output the parameters $a_0,a_1,a_2$ of the regression plane. We will need to use numpy's linear algebra capabilities to solve the matrix equation that gives us the solution.\n",
    "\n",
    "In other words, our function will perform multiple regression as it is done in the example form the class. This is just a practice example. In reality we would not implement this function like this, as it is limited to just 2 indepenedent variables. For more flexibility we would implement the general method using the formula $A = (Z^TZ)^{-1}Z^TY$ (see the class material, and also later in this notebook), or depending on the data we have it could be faster to solve the optimization problem numerically using something like gradient descent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mRegression(x_values, y_values):\n",
    "    # We need to populate the matrix from the notes with sums of x values.\n",
    "    x_values = np.array(x_values)\n",
    "    y_values = np.array(y_values)\n",
    "    n = len(y_values)\n",
    "    Sx_1 = np.sum(x_values,0)[0]\n",
    "    Sx_2 = np.sum(x_values,0)[1]\n",
    "    Sx_1x_2 = np.sum(x_values[:,0]*x_values[:,1])\n",
    "    Sx_1_square = np.sum(x_values[:,0]**2)\n",
    "    Sx_2_square = np.sum(x_values[:,1]**2)\n",
    "    Sy = np.sum(y_values)\n",
    "    Sx_1y = np.sum(x_values[:,0]*y_values)\n",
    "    Sx_2y = np.sum(x_values[:,1]*y_values)\n",
    "    M = np.array([[n, Sx_1, Sx_2], [Sx_1, Sx_1_square, Sx_1x_2], [Sx_2, Sx_1x_2, Sx_2_square]])\n",
    "    col = [Sy, Sx_1y, Sx_2y]    \n",
    "    return np.linalg.solve(M, col)\n",
    "\n",
    "# We can check this gives the same answer as we got using model.fit earlier.\n",
    "print(mRegression(X_2,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will tackle a general regression problem. \n",
    "\n",
    "Suppose the number of particles in a system decays exponentially according to the model \n",
    "\n",
    "$$p(t) = ae^{-1.5t} + be^{-0.3t}+ce^{-0.05t}.$$\n",
    "\n",
    "Here $p(t)$ is the number of particles remaining at time $t$. \n",
    "\n",
    "We want to find the values $a,b,c$ so we fully understand this model. To do this we run an experiment and collect data at various times. This is given in the following table:\n",
    "\n",
    "|  t  |  p(t)  |\n",
    "|-----|--------|\n",
    "| 0.5 | 6      |\n",
    "| 1   | 4.4    |\n",
    "| 2   | 3.2    |\n",
    "| 3   | 2.7    |\n",
    "| 4   | 2      |\n",
    "| 5   | 1.9    |\n",
    "| 6   | 1.7    |\n",
    "| 7   | 1.4    |\n",
    "| 9   | 1.1    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this we will use the general least squares regression method. Here $y = az_0 + bz_1 + cz_2$, where $z_0, z_1$, and $z_2$ are 'basis functions' of $t$.\n",
    "\n",
    "In this case $z_0 = e^{-1.5t}$, $z_1 = e^{-0.3t}$ and $z_2 = e^{-0.05t}$.\n",
    "\n",
    "Using these basis functions and the values of $t$ given in the data we create a matrix $Z$.\n",
    "We also get a vector $Y$ from the values of $p(t)$.\n",
    "We find the values of $a,b,c$ by evaluating the matrix formula $(Z^TZ)^{-1}Z^TY$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "t_values = [0.5, 1, 2, 3, 4, 5, 6, 7, 9]\n",
    "Y = np.array([[6], [4.4], [3.2], [2.7], [2], [1.9], [1.7], [1.4], [1.1]])\n",
    "\n",
    "# Returns the matrix Z as an array. \n",
    "# You need to write this part.\n",
    "# You can find information about Z in the notes.\n",
    "def make_Z(t = t_values):\n",
    "    # TODO\n",
    "    return np.array(A)\n",
    "\n",
    "\n",
    "# Now using the matrix Z we can evaluate a matrix formula to find the parameters a,b,c.\n",
    "Z = make_Z()\n",
    "Zt = np.transpose(Z)\n",
    "A = np.matmul(np.matmul(np.linalg.inv(np.matmul(Zt,Z)),Zt),Y)\n",
    "print(A)\n",
    "\n",
    "\n",
    "# If my calculation is correct, this should produce \n",
    "# [[4.13749658]\n",
    "#  [2.89588175]\n",
    "#  [1.53491995]]\n",
    "# This corresponds to a = 4.137, b = 2.896, c = 1.535 (to 3 decimal places). \n",
    "# This gives us an estimate for the parameters of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test how good this model is by comparing the values predicted if you use these values for $a,b,c$ with the model for $p(t)$ at the $t$ values given in the data with the observed values of $p(t)$. E.g. you can calcuate $S_t$ and $S_r$, and so also $r^2$ and $s_{yx}$. We will just draw a picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots() \n",
    "ax.scatter(t_values, Y.flatten(), c = 'orange')\n",
    "A = np.array([[4.13749658],  [2.89588175], [1.53491995]])\n",
    "a,b,c = A.flatten()\n",
    "x_line = np.linspace(0,10,100)\n",
    "y_line = a*np.exp(-1.5*x_line) + b*np.exp(-0.3*x_line) + c*np.exp(-0.05*x_line) \n",
    "# we needed the numpy exp function in the line above to draw the line properly\n",
    "plt.plot(x_line, y_line)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
